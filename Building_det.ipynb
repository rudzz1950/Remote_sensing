{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Project Architecture\n"
      ],
      "metadata": {
        "id": "mwqcrutT_ohz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie5uLDH4uzAp"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/ultralytics.git  # Clone YOLOv8 repo\n",
        "!cd ultralytics\n",
        "\n",
        "!pip install ultralytics\n",
        "!pip install torch torchvision numpy opencv-python matplotlib grad-cam ultralytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Input image\n",
        "https://drive.google.com/file/d/1HFFAzC9S3yl8BtZ0WZGwt5h_HuK8OHdx/view?usp=sharing\n",
        "\n",
        "2. Output image\n",
        "https://drive.google.com/file/d/1fYcPXh60cLzIE1CLh0k6yNe89TAR7Ja6/view?usp=sharing"
      ],
      "metadata": {
        "id": "6I9Ljbdu_ZVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "ECieZUKN-i8k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbvMlHd_QwMG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Check PyTorch and CUDA\n",
        "device = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
        "print(f\"Setup complete. Using torch {torch.__version__} on {device}\")\n",
        "\n",
        "# Load a YOLOv8 model to test\n",
        "model = YOLO(\"yolov8m.pt\")  # Load the nano model\n",
        "print(\"YOLOv8 model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knxi2ncxWffW"
      },
      "outputs": [],
      "source": [
        "# Install Roboflow\n",
        "!pip install roboflow\n",
        "\n",
        "# Import required libraries\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Initialize Roboflow with API key\n",
        "rf = Roboflow(api_key=\"iMVOMaxCVf9Q6wQNbSnb\")\n",
        "\n",
        "# Load the original house_alloc project\n",
        "project_house = rf.workspace(\"quantela\").project(\"house_alloc\")\n",
        "version_house = project_house.version(17)\n",
        "dataset_house = version_house.download(\"yolov8\")\n",
        "print(\"House allocation dataset downloaded successfully!\")\n",
        "\n",
        "# Load the new tree-seg project\n",
        "project_tree = rf.workspace(\"test-4udyq\").project(\"tree-seg\")\n",
        "version_tree = project_tree.version(1)  # Assuming version 1, adjust if different\n",
        "dataset_tree = version_tree.download(\"yolov8\")\n",
        "print(\"Tree segmentation dataset downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define base paths for both datasets\n",
        "base_path_house = '/content/house_alloc-17'  # Adjust to match your house dataset\n",
        "base_path_tree = '/content/tree-seg-1'      # Adjust to match your tree dataset\n",
        "\n",
        "# Define all required subdirectories\n",
        "subdirs = [\n",
        "    'train/images',\n",
        "    'train/labels',\n",
        "    'valid/images',\n",
        "    'valid/labels',\n",
        "    'test/images',\n",
        "    'test/labels'\n",
        "]\n",
        "\n",
        "# Create directories for house dataset\n",
        "try:\n",
        "    for subdir in subdirs:\n",
        "        dir_path = os.path.join(base_path_house, subdir)\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "    print(\"Directories for house YOLOv8 dataset created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to create house directories: {e}\")\n",
        "\n",
        "# Create directories for tree dataset\n",
        "try:\n",
        "    for subdir in subdirs:\n",
        "        dir_path = os.path.join(base_path_tree, subdir)\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "    print(\"Directories for tree YOLOv8 dataset created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to create tree directories: {e}\")\n",
        "\n",
        "# Verify the structure\n",
        "print(\"Created directory structure for house dataset:\")\n",
        "for subdir in subdirs:\n",
        "    print(f\"- {os.path.join(base_path_house, subdir)}\")\n",
        "\n",
        "print(\"Created directory structure for tree dataset:\")\n",
        "for subdir in subdirs:\n",
        "    print(f\"- {os.path.join(base_path_tree, subdir)}\")"
      ],
      "metadata": {
        "id": "kiqfSv9_NMTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_path = '/content/house_alloc-17'  # Match your Roboflow download"
      ],
      "metadata": {
        "id": "tFg0dQHjNdBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "2VlX-RKzN4It"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ3DmmGQztJj"
      },
      "outputs": [],
      "source": [
        "# # Fix locale encoding\n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# # Your original code\n",
        "# dataset_location = dataset.location\n",
        "# %cat {dataset_location}/data.yaml\n",
        "\n",
        "# # This is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "# dataset_location = dataset.location\n",
        "# %cat {dataset_location}/data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOPn9wjOAwwK"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Fix locale encoding\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# Dataset locations\n",
        "dataset_location_house = dataset_house.location  # e.g., '/content/house_alloc-17'\n",
        "dataset_location_tree = dataset_tree.location    # e.g., '/content/tree-seg-1'\n",
        "\n",
        "# YAML paths\n",
        "yaml_path_house = os.path.join(dataset_location_house, 'data.yaml').replace(\"\\\\\", \"/\")\n",
        "yaml_path_tree = os.path.join(dataset_location_tree, 'data.yaml').replace(\"\\\\\", \"/\")\n",
        "\n",
        "# Update house YAML\n",
        "with open(yaml_path_house, 'r') as f:\n",
        "    data_house = yaml.safe_load(f)\n",
        "data_house['train'] = 'train/images'\n",
        "data_house['val'] = 'valid/images'\n",
        "data_house['test'] = 'test/images'\n",
        "with open(yaml_path_house, 'w') as f:\n",
        "    yaml.safe_dump(data_house, f)\n",
        "print(\"House YAML updated!\")\n",
        "\n",
        "# Update tree YAML\n",
        "with open(yaml_path_tree, 'r') as f:\n",
        "    data_tree = yaml.safe_load(f)\n",
        "data_tree['train'] = 'train/images'\n",
        "data_tree['val'] = 'valid/images'\n",
        "data_tree['test'] = 'test/images'\n",
        "with open(yaml_path_tree, 'w') as f:\n",
        "    yaml.safe_dump(data_tree, f)\n",
        "print(\"Tree YAML updated!\")\n",
        "\n",
        "# Verify\n",
        "print(\"House YAML:\")\n",
        "!cat {yaml_path_house}\n",
        "print(\"Tree YAML:\")\n",
        "!cat {yaml_path_tree}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/ultralytics/ultralytics/cfg/models/v8/"
      ],
      "metadata": {
        "id": "b5g3gkf7T21k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rvt5wilnDyX"
      },
      "outputs": [],
      "source": [
        "# This is the model configuration we will use for our tutorial\n",
        "%cat /content/ultralytics/ultralytics/cfg/models/v8/yolov8-seg.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDxebz13RdRA"
      },
      "outputs": [],
      "source": [
        "# Register the custom writetemplate magic\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))\n",
        "\n",
        "# Set number of classes\n",
        "num_classes_house = 1  # For 'Houses' from house_alloc-17\n",
        "num_classes_tree = 1   # Adjust based on tree-seg dataset (check data.yaml for 'names')\n",
        "\n",
        "# House YOLOv8m custom config\n",
        "config_text_house = f\"\"\"# YOLOv8 medium custom config for house detection\n",
        "nc: {num_classes_house}  # Number of classes (1 for Houses)\n",
        "depth_multiple: 0.67  # Model depth multiple (medium scale)\n",
        "width_multiple: 0.75  # Layer channel multiple (medium scale)\n",
        "\n",
        "# Backbone\n",
        "backbone:\n",
        "  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n",
        "  - [-1, 3, C2f, [128, True]]   # 2\n",
        "  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n",
        "  - [-1, 6, C2f, [256, True]]   # 4\n",
        "  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n",
        "  - [-1, 6, C2f, [512, True]]   # 6\n",
        "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
        "  - [-1, 3, C2f, [1024, True]]  # 8\n",
        "  - [-1, 1, SPPF, [1024, 5]]    # 9 (SPP block)\n",
        "\n",
        "# Head\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 10\n",
        "  - [[-1, 6], 1, Concat, [1]]  # 11 cat backbone P4\n",
        "  - [-1, 3, C2f, [512, True]]   # 12\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 13\n",
        "  - [[-1, 4], 1, Concat, [1]]  # 14 cat backbone P3\n",
        "  - [-1, 3, C2f, [256, True]]   # 15 (P3/8-small)\n",
        "  - [-1, 1, Conv, [256, 3, 2]]  # 16\n",
        "  - [[-1, 12], 1, Concat, [1]]  # 17 cat head P4\n",
        "  - [-1, 3, C2f, [512, True]]   # 18 (P4/16-medium)\n",
        "  - [-1, 1, Conv, [512, 3, 2]]  # 19\n",
        "  - [[-1, 9], 1, Concat, [1]]  # 20 cat head P5\n",
        "  - [-1, 3, C2f, [1024, True]]  # 21 (P5/32-large)\n",
        "  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)\n",
        "\"\"\"\n",
        "\n",
        "# Tree YOLOv8m segmentation custom config\n",
        "config_text_tree = f\"\"\"# YOLOv8 medium custom config for tree segmentation\n",
        "nc: {num_classes_tree}  # Number of classes (e.g., 1 for Trees)\n",
        "depth_multiple: 0.67  # Model depth multiple (medium scale)\n",
        "width_multiple: 0.75  # Layer channel multiple (medium scale)\n",
        "\n",
        "# Backbone (same as detection)\n",
        "backbone:\n",
        "  - [-1, 1, Conv, [64, 3, 2]]   # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n",
        "  - [-1, 3, C2f, [128, True]]   # 2\n",
        "  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n",
        "  - [-1, 6, C2f, [256, True]]   # 4\n",
        "  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n",
        "  - [-1, 6, C2f, [512, True]]   # 6\n",
        "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
        "  - [-1, 3, C2f, [1024, True]]  # 8\n",
        "  - [-1, 1, SPPF, [1024, 5]]    # 9 (SPP block)\n",
        "\n",
        "# Head (modified for segmentation)\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 10\n",
        "  - [[-1, 6], 1, Concat, [1]]  # 11 cat backbone P4\n",
        "  - [-1, 3, C2f, [512, True]]   # 12\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 13\n",
        "  - [[-1, 4], 1, Concat, [1]]  # 14 cat backbone P3\n",
        "  - [-1, 3, C2f, [256, True]]   # 15 (P3/8-small)\n",
        "  - [-1, 1, Conv, [256, 3, 2]]  # 16\n",
        "  - [[-1, 12], 1, Concat, [1]]  # 17 cat head P4\n",
        "  - [-1, 3, C2f, [512, True]]   # 18 (P4/16-medium)\n",
        "  - [-1, 1, Conv, [512, 3, 2]]  # 19\n",
        "  - [[-1, 9], 1, Concat, [1]]   # 20 cat head P5\n",
        "  - [-1, 3, C2f, [1024, True]]  # 21 (P5/32-large)\n",
        "  - [[15, 18, 21], 1, Segment, [nc, 32, 256]]  # Segment(P3, P4, P5) - Segmentation head\n",
        "\"\"\"\n",
        "\n",
        "# Save the config files\n",
        "config_path_house = \"/content/yolov8m-custom-house.yaml\"\n",
        "with open(config_path_house, \"w\") as f:\n",
        "    f.write(config_text_house)\n",
        "print(f\"House config file saved at {config_path_house}\")\n",
        "\n",
        "config_path_tree = \"/content/yolov8m-custom-tree.yaml\"\n",
        "with open(config_path_tree, \"w\") as f:\n",
        "    f.write(config_text_tree)\n",
        "print(f\"Tree config file saved at {config_path_tree}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSPQthWMMzjI"
      },
      "outputs": [],
      "source": [
        "# Update the Ultralytics repo for YOLOv8\n",
        "!cd /content/ultralytics && git pull\n",
        "\n",
        "print(dataset_location_house)\n",
        "print(dataset_location_tree)\n",
        "\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train house model\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load pretrained YOLOv8m model for house detection\n",
        "model_house = YOLO(\"yolov8m-custom-house.yaml\").load(\"yolov8m.pt\")  # Load with pre-trained weights\n",
        "\n",
        "# Train the house model\n",
        "model_house.train(\n",
        "    data=f\"{dataset_location_house}/data.yaml\",  # Path to house dataset config\n",
        "    epochs=20,                            # Reduced for demo; increase for better results\n",
        "    imgsz=640,                           # Larger image size for better detection\n",
        "    batch=16,                            # Batch size suitable for Tesla T4 GPU\n",
        "    patience=20,                         # Early stopping after 5 epochs of no improvement\n",
        "    cache=\"disk\",                        # Use disk caching for deterministic results\n",
        "    device=0,                            # Use GPU (Tesla T4, CUDA:0)\n",
        "    workers=4,                           # Number of data loader workers\n",
        "    pretrained=True,                     # Explicitly use pretrained weights\n",
        "    lr0=0.0005,                          # Lower initial learning rate for stability\n",
        "    mosaic=1.0,\n",
        "    augment=True,                        # Enable data augmentation\n",
        "    name=\"yolov8m_house_results\",\n",
        ")\n",
        "\n",
        "# Train tree model\n",
        "model_tree = YOLO(\"yolov8m-custom-tree.yaml\").load(\"yolov8m-seg.pt\")  # Load with pre-trained weights\n",
        "\n",
        "# Train the tree model\n",
        "model_tree.train(\n",
        "    data=f\"{dataset_location_tree}/data.yaml\",  # Path to tree dataset config\n",
        "    epochs=20 ,                            # Reduced for demo; increase for better results\n",
        "    imgsz=640,                           # Larger image size for better detection\n",
        "    batch=16,                            # Batch size suitable for Tesla T4 GPU\n",
        "    patience=5,                         # Early stopping after 5 epochs of no improvement\n",
        "    cache=\"disk\",                        # Use disk caching for deterministic results\n",
        "    device=0,                            # Use GPU (Tesla T4, CUDA:0)\n",
        "    workers=4,                           # Number of data loader workers\n",
        "    pretrained=True,                     # Explicitly use pretrained weights\n",
        "    lr0=0.0005,                          # Lower initial learning rate for stability\n",
        "    mosaic=1.0,\n",
        "    augment=True,                        # Enable data augmentation\n",
        "    name=\"yolov8m_tree_results\",\n",
        ")\n",
        "\n",
        "# Optional: Validate both models after training\n",
        "model_house.val()  # Runs validation on the best saved weights for house\n",
        "model_tree.val()   # Runs validation on the best saved weights for tree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "osfLVRg5Eg_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "img_path = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "print(\"Using file:\", img_path)"
      ],
      "metadata": {
        "id": "Ex7N1CXMEaxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load the input image using PIL and convert to RGB\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "image_np = np.array(image)  # Convert to numpy array for processing\n",
        "\n",
        "# Load trained YOLO models\n",
        "model_house = YOLO(\"runs/detect/yolov8m_house_results/weights/best.pt\")  # House detection model\n",
        "# model_tree = YOLO(\"runs/detect/yolov8m_tree_results/weights/best.pt\")  # Tree segmentation model\n",
        "\n",
        "# Download and load the building segmentation model from Hugging Face\n",
        "model_path = hf_hub_download(repo_id=\"keremberke/yolov8m-building-segmentation\", filename=\"best.pt\")\n",
        "model_building = YOLO(model_path)  # Building segmentation model\n",
        "\n",
        "# Run inference for all three models\n",
        "results_house = model_house(image_np)\n",
        "# results_tree = model_tree(image_np)\n",
        "results_building = model_building(image_np)\n",
        "\n",
        "# Define output directory inside Colab\n",
        "output_dir = \"/content/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create folder if it doesn’t exist\n",
        "\n",
        "# Extract original filename without extension\n",
        "base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "output_path_combined = os.path.join(output_dir, f\"{base_name}_combined_output.jpg\")\n",
        "\n",
        "# Create a copy of the original image for combined output\n",
        "combined_image = image_np.copy()\n",
        "\n",
        "# Process house detection results (bounding boxes)\n",
        "if results_house and results_house[0].boxes is not None:  # Check if there are detections\n",
        "    for r in results_house:\n",
        "        boxes = r.boxes.xyxy.cpu().numpy()  # [x_min, y_min, x_max, y_max]\n",
        "        confidences = r.boxes.conf.cpu().numpy()\n",
        "        class_ids = r.boxes.cls.cpu().numpy()\n",
        "\n",
        "        for box, conf, cls_id in zip(boxes, confidences, class_ids):\n",
        "            x_min, y_min, x_max, y_max = map(int, box)\n",
        "            # Draw house bounding box in blue\n",
        "            cv2.rectangle(combined_image, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)  # Blue color\n",
        "            label = f\"House: {conf:.2f}\"\n",
        "            cv2.putText(combined_image, label, (x_min, y_min - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "# # Process tree segmentation results (masks)\n",
        "# if results_tree and results_tree[0].masks is not None:  # Check if there are segmentation masks\n",
        "#     for r in results_tree:\n",
        "#         masks = r.masks.data.cpu().numpy()  # Segmentation masks\n",
        "#         boxes = r.boxes.xyxy.cpu().numpy() if r.boxes is not None else None  # Bounding boxes (optional)\n",
        "#         confidences = r.boxes.conf.cpu().numpy() if r.boxes is not None else None\n",
        "\n",
        "#         for i, mask in enumerate(masks):\n",
        "#             # Resize mask to match the input image dimensions\n",
        "#             mask_resized = cv2.resize(mask, (combined_image.shape[1], combined_image.shape[0]),\n",
        "#                                      interpolation=cv2.INTER_NEAREST)\n",
        "#             mask_binary = (mask_resized > 0).astype(np.uint8) * 255\n",
        "\n",
        "#             # Create a green overlay for the tree mask\n",
        "#             colored_mask = np.zeros_like(combined_image)\n",
        "#             colored_mask[:, :, 1] = mask_binary  # Green channel\n",
        "#             combined_image = cv2.addWeighted(combined_image, 0.8, colored_mask, 0.5, 0)\n",
        "\n",
        "#             # Optionally, add a label using the bounding box (if available)\n",
        "#             if boxes is not None and confidences is not None and i < len(boxes):\n",
        "#                 x_min, y_min, x_max, y_max = map(int, boxes[i])\n",
        "#                 label = f\"Tree: {confidences[i]:.2f}\"\n",
        "#                 cv2.putText(combined_image, label, (x_min, y_min - 10),\n",
        "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "# Process building segmentation results (masks)\n",
        "if results_building and results_building[0].masks is not None:  # Check if there are segmentation masks\n",
        "    for r in results_building:\n",
        "        masks = r.masks.data.cpu().numpy()  # Segmentation masks\n",
        "        boxes = r.boxes.xyxy.cpu().numpy() if r.boxes is not None else None  # Bounding boxes (optional)\n",
        "        confidences = r.boxes.conf.cpu().numpy() if r.boxes is not None else None\n",
        "\n",
        "        for i, mask in enumerate(masks):\n",
        "            # Resize mask to match the input image dimensions\n",
        "            mask_resized = cv2.resize(mask, (combined_image.shape[1], combined_image.shape[0]),\n",
        "                                     interpolation=cv2.INTER_NEAREST)\n",
        "            mask_binary = (mask_resized > 0).astype(np.uint8) * 255\n",
        "\n",
        "            # Create a red overlay for the building mask\n",
        "            colored_mask = np.zeros_like(combined_image)\n",
        "            colored_mask[:, :, 2] = mask_binary  # Red channel\n",
        "            combined_image = cv2.addWeighted(combined_image, 0.8, colored_mask, 0.5, 0)\n",
        "\n",
        "            # Optionally, add a label using the bounding box (if available)\n",
        "            if boxes is not None and confidences is not None and i < len(boxes):\n",
        "                x_min, y_min, x_max, y_max = map(int, boxes[i])\n",
        "                label = f\"Building: {confidences[i]:.2f}\"\n",
        "                cv2.putText(combined_image, label, (x_min, y_min - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "# Convert combined image back to PIL format and save\n",
        "combined_image_pil = Image.fromarray(combined_image)\n",
        "combined_image_pil.save(output_path_combined, format=\"JPEG\", quality=95)\n",
        "\n",
        "# Display the combined result\n",
        "plt.figure(figsize=(12, 12))  # Increase figure size for better visibility\n",
        "#plt.imshow(combined_image)\n",
        "#plt.axis(\"off\")\n",
        "#plt.title(\"Combined House Detection, Tree Segmentation, and Building Segmentation\")\n",
        "#\n",
        "plt.show()\n",
        "print(f\"Combined result saved at {output_path_combined}\")\n",
        "\n",
        "# Download the combined image to your local machine\n",
        "files.download(output_path_combined)"
      ],
      "metadata": {
        "id": "Ab-PK1fZXfZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Metrics"
      ],
      "metadata": {
        "id": "O7gjjJlVfeXP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVs_4zEeVbF"
      },
      "source": [
        "# Evaluate Custom YOLOv8 Detector Performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov8s_results_80`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.\n",
        "\n",
        "Note from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOy5KI2ncnWd"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}